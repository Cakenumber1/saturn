{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Лаб 5 М33081 Николаев \"Text_processing_webinar.ipynb\"\"",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1hqb2oWg96"
      },
      "source": [
        "# Инструменты для работы с текстом"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXhKw6r2Wg97"
      },
      "source": [
        "Анализ текстовых данных - это отдельное направление, здесь будет совсем небольшое введение.\n",
        "С текстовыми данными можно решать как задачи обучения с учителем (классификация текстов), так и задачу обучения без учителя (кластеризация).\n",
        "\n",
        "Предобработка текста\n",
        "\n",
        "Первый шаг любой аналитики – получение данных. Предположим, что данные представляются собой набор текстов. Все известные нам алгоритмы работают не в текстами, а с объектами, которые описываются вектором признаков (чаще всего численных, категориальные мы умеем преобразовывать). Что делать, если наши объекты - это текст? \n",
        "\n",
        "Следующая после получения данных задача: предобработка. Основная цель предобработки: преобразовать текстовые данные в удобный для построения модели вид.\n",
        "\n",
        "Базовые шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. удаление пунктуации\n",
        "5. фильтрация по частоте/длине/соответствию регулярному выражению\n",
        "6. лемматизация или стемминг\n",
        "7. векторизация (эмбеддинг)\n",
        "\n",
        "Чаще всего применяются все эти шаги, но в разных задачах какие-то могут опускаться, поскольку приводят к потере информации"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX-AeH8RWg9-"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpq4QOU5Wg-H"
      },
      "source": [
        "* Сейчас мы попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения. \n",
        "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
        "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7DyyXRWg-K"
      },
      "source": [
        "## n-граммы\n",
        "\n",
        "Самые мелкие структуры языка, с которыми мы работаем, называются **n-граммами**.\n",
        "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
        "* Если n = 1 - то мы смотрим на то, сколько раз каждое слово встретилось в тексте. Получаем _униграммы_\n",
        "* Если n = 2 - то мы смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаем _биграммы_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiUoyqNb3WA"
      },
      "source": [
        "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcrWxBzzWg-K"
      },
      "source": [
        "from nltk import ngrams"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ib-zYTvfQq5"
      },
      "source": [
        "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJ9aYx2UPK44"
      },
      "source": [
        "sentence = 'Кто же победит на выборах в США: Трамп или Байден?'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2Ql8Em4Wg-N",
        "outputId": "c39c2d62-a6a2-4b34-bf1c-63ba6bb3dca6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence_split = sentence.split()\n",
        "sentence_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто',\n",
              " 'же',\n",
              " 'победит',\n",
              " 'на',\n",
              " 'выборах',\n",
              " 'в',\n",
              " 'США:',\n",
              " 'Трамп',\n",
              " 'или',\n",
              " 'Байден?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 476
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIvz7OLN8yV"
      },
      "source": [
        "Кажется, что нам тут мешают знаки препинания. Дайвайте от них избавимся. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i3IvCTPOIfB",
        "outputId": "b900deb0-a5c9-4e27-938b-a62590c9eb55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 477
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NTiPD_lfPERo"
      },
      "source": [
        "for ch in string.punctuation:\n",
        "  sentence = sentence.replace(ch,\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eY7anMcPxGB",
        "outputId": "a4626ee7-80ea-46a6-b6a6-4e79da2191fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sentence"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Кто же победит на выборах в США Трамп или Байден'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 479
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWvXpEdDP4kI"
      },
      "source": [
        "sentence_split = sentence.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8uy1TrfAP-ui",
        "outputId": "99667814-98bc-46da-fc3a-b7a30956c8fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence_split"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто', 'же', 'победит', 'на', 'выборах', 'в', 'США', 'Трамп', 'или', 'Байден']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 481
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6V5P2Jcc4Oy"
      },
      "source": [
        "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
        "\n",
        "На вход передается два параметра:\n",
        "* лист с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sent```);\n",
        "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
        "\n",
        "\n",
        "Чтобы полученный объект отобразить, делаем из него ```list```. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9oqpykUc5e9",
        "outputId": "67f47efd-4ee6-47a9-fc59-ba7ccfdfe276",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(ngrams(sentence_split, 1)) # униграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто',),\n",
              " ('же',),\n",
              " ('победит',),\n",
              " ('на',),\n",
              " ('выборах',),\n",
              " ('в',),\n",
              " ('США',),\n",
              " ('Трамп',),\n",
              " ('или',),\n",
              " ('Байден',)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 482
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZKRhRlxfoj4"
      },
      "source": [
        "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bzl6t5dpWg-P",
        "outputId": "a06e86db-cb9b-4ba2-9d62-e78bdabe4757",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(ngrams(sentence_split, 2)) # биграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто', 'же'),\n",
              " ('же', 'победит'),\n",
              " ('победит', 'на'),\n",
              " ('на', 'выборах'),\n",
              " ('выборах', 'в'),\n",
              " ('в', 'США'),\n",
              " ('США', 'Трамп'),\n",
              " ('Трамп', 'или'),\n",
              " ('или', 'Байден')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 483
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nCkkFzWLWg-R",
        "outputId": "1725817d-fb0b-4815-a2e9-99d19e657033",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(ngrams(sentence_split, 3)) # триграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто', 'же', 'победит'),\n",
              " ('же', 'победит', 'на'),\n",
              " ('победит', 'на', 'выборах'),\n",
              " ('на', 'выборах', 'в'),\n",
              " ('выборах', 'в', 'США'),\n",
              " ('в', 'США', 'Трамп'),\n",
              " ('США', 'Трамп', 'или'),\n",
              " ('Трамп', 'или', 'Байден')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 484
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygS6_fJWg-S",
        "outputId": "031c87d1-8166-40a8-9565-612c8ed8080e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(ngrams(sentence_split, 5)) # ... пентаграммы"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Кто', 'же', 'победит', 'на', 'выборах'),\n",
              " ('же', 'победит', 'на', 'выборах', 'в'),\n",
              " ('победит', 'на', 'выборах', 'в', 'США'),\n",
              " ('на', 'выборах', 'в', 'США', 'Трамп'),\n",
              " ('выборах', 'в', 'США', 'Трамп', 'или'),\n",
              " ('в', 'США', 'Трамп', 'или', 'Байден')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 485
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JewKs4XU-so"
      },
      "source": [
        "## Векторизаторы\n",
        "\n",
        "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
        "\n",
        "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hiNv2eVAc-"
      },
      "source": [
        "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в библиотеке **sklearn**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPplZnxeVEBR"
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier # можно заменить на другой классификатор\n",
        "from sklearn.naive_bayes import MultinomialNB # наивный байесовский классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer # модель \"мешка слов\", см. далее"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBN16KYZWg-U"
      },
      "source": [
        "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает следующую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1jHmkrGZTMawM46Yzxh243Ur1y5pYKzrl\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklbwY_vWg-X"
      },
      "source": [
        "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5KabRubaXhNb"
      },
      "source": [
        "## Пример"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EnHNZtbXlH0"
      },
      "source": [
        "К сожалению, на русском языке всё ещё очень мало годных наборов данных. Набор данных нашёл тут: https://github.com/sismetanin/rureviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yX-0fRF3hzCy",
        "outputId": "89072f85-1f65-464f-89e8-1a806f9bf4f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install PyDrive"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (1.7.12)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from PyDrive) (3.13)\n",
            "Requirement already satisfied: six>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (4.6)\n",
            "Requirement already satisfied: httplib2>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.17.2)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (4.1.1)\n",
            "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->PyDrive) (50.3.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aYRCckjEh2--"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBBer1ozh5mL"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bP_tPtQDiGCx"
      },
      "source": [
        "downloaded = drive.CreateFile({'id':\"1pvDGJWmxTfJ9VAi4mGb5ByOWMlP3hNFX\"}) \n",
        "downloaded.GetContentFile('women-clothing-accessories.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ3jSWXe631D"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6709r3nXwIu"
      },
      "source": [
        "data = pd.read_csv('women-clothing-accessories.csv', sep='\\t', usecols=[0, 1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u894TFlsYq3o",
        "outputId": "2bd58fa0-4d6e-4645-b29b-1e726410af27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              review sentiment\n",
              "0  качество плохое пошив ужасный (горловина напер...  negative\n",
              "1  Товар отдали другому человеку, я не получила п...  negative\n",
              "2  Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
              "3  товар не пришел, продавец продлил защиту без м...  negative\n",
              "4      Кофточка голая синтетика, носить не возможно.  negative"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 492
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09aeAe4zvwCv",
        "outputId": "64dcec87-31c2-4abf-e829-14524019203a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        }
      },
      "source": [
        "data.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>7633</th>\n",
              "      <td>Это ужасный товар. Пахнет клеем, материал не п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84656</th>\n",
              "      <td>XXXL-на 50размер</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>53928</th>\n",
              "      <td>Размеру не соответствует, Брала xl на 48р,оказ...</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11684</th>\n",
              "      <td>Отправил заказ не на тот адрес. Деньги за това...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1876</th>\n",
              "      <td>Не пришла, инфо об отслеживании не было</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84763</th>\n",
              "      <td>Рубашка хорошая, но слегка короче чем ожидалось</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49059</th>\n",
              "      <td>Сначала заказала размер М. Оказались большие. ...</td>\n",
              "      <td>neautral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69685</th>\n",
              "      <td>Быстрая доставка до РБ. Отличная футболочка. В...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>65638</th>\n",
              "      <td>качество хорошее теплая, но выглядит как обычн...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>80507</th>\n",
              "      <td>Ткань не очень и цвет очень сильно отличается</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "7633   Это ужасный товар. Пахнет клеем, материал не п...  negative\n",
              "84656                                   XXXL-на 50размер  positive\n",
              "53928  Размеру не соответствует, Брала xl на 48р,оказ...  neautral\n",
              "11684  Отправил заказ не на тот адрес. Деньги за това...  negative\n",
              "1876             Не пришла, инфо об отслеживании не было  negative\n",
              "84763   Рубашка хорошая, но слегка короче чем ожидалось   positive\n",
              "49059  Сначала заказала размер М. Оказались большие. ...  neautral\n",
              "69685  Быстрая доставка до РБ. Отличная футболочка. В...  positive\n",
              "65638  качество хорошее теплая, но выглядит как обычн...  positive\n",
              "80507      Ткань не очень и цвет очень сильно отличается  positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 493
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpWqBU3-ZL5F"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(data.review, data.sentiment, train_size = 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKB-qv_obPrD",
        "outputId": "8e3aa404-3590-49c9-9e8a-2151bd98fb1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "data.sentiment.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "negative    30000\n",
              "positive    30000\n",
              "neautral    30000\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 495
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtowE76LbIrG",
        "outputId": "f1928a66-e326-4488-b419-bab65f7340dd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "y_train.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "neautral    21015\n",
              "negative    21002\n",
              "positive    20982\n",
              "Name: sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 496
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muRPBnySwfih"
      },
      "source": [
        "Инициализируем `CountVectorizer()`, указав в качестве признаков униграммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfavuPF9wZh_"
      },
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIxErQg4wm-H"
      },
      "source": [
        "После инициализации _vectorizer_ можно обучить на наших данных. \n",
        "\n",
        "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к нашему набору данных. Это похоже на то, как мы работали с one-hot-encoderом."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iwq0gTww0Pw"
      },
      "source": [
        "vectorized_x_train = vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Scw-UGKrwysI"
      },
      "source": [
        "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXZEX-AxxDp4"
      },
      "source": [
        "В `vectorizer.vocabulary_` лежит словарь, отображение слов в их индексы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRohdVRSxJIa",
        "outputId": "353de4d2-4052-4b20-9a61-5e43a7893c69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(vectorizer.vocabulary_.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('заказывала', 12649),\n",
              " ('31', 724),\n",
              " ('размер', 33844),\n",
              " ('пришел', 31777),\n",
              " ('кажется', 14901),\n",
              " ('25', 568),\n",
              " ('или', 14425),\n",
              " ('26', 582),\n",
              " ('очень', 25848),\n",
              " ('маленький', 18200)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 499
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQCbnAk4wS_K",
        "outputId": "170396a6-21d9-43d4-fa0b-dc7abe4c9975",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999, 44880)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 500
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKFQVUZ11K47",
        "outputId": "b581d39f-ef2c-45b0-b7f9-8061c20bbd0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 501
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXAcWenlxQfc"
      },
      "source": [
        "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить нашу модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icUoj9EexWcs",
        "outputId": "08a10f6b-1e73-4adf-9d0c-0c5acfd4446a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf = DecisionTreeClassifier()\n",
        "clf.fit(vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 502
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf9V0s6jI1l2",
        "outputId": "7b038559-c913-4e3f-925c-158693168acf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "clf2 = MultinomialNB()\n",
        "clf2.fit(vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 503
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jM85715BxfIx"
      },
      "source": [
        "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
        "\n",
        "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWX6X7UHxjkj"
      },
      "source": [
        "vectorized_x_test = vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzvSUzEcxqMo"
      },
      "source": [
        "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
        "\n",
        "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7gssLYYxwkK",
        "outputId": "6ea34985-14e6-4773-e03f-12ded9c0fbce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred = clf.predict(vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.52      0.54      0.53      8985\n",
            "    negative       0.64      0.62      0.63      8998\n",
            "    positive       0.75      0.75      0.75      9018\n",
            "\n",
            "    accuracy                           0.64     27001\n",
            "   macro avg       0.64      0.64      0.64     27001\n",
            "weighted avg       0.64      0.64      0.64     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvtntoJ0JNoK",
        "outputId": "504a4c59-6b0f-48e4-e06d-bfba38de6d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "pred2 = clf2.predict(vectorized_x_test)\n",
        "print(classification_report(y_test, pred2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.68      0.63      8985\n",
            "    negative       0.73      0.62      0.67      8998\n",
            "    positive       0.84      0.84      0.84      9018\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seQ_QrKmJWoF"
      },
      "source": [
        "Итак, наивный байесовский классификатор легко побил дерево решений. Дальше работаем с ним."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qcO2BceNsv"
      },
      "source": [
        "### Отступление: F-мера"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_qn9NrqeW6w"
      },
      "source": [
        "Прошлый раз мы разобрали метрики качества классификации, которые выводятся из матрицы ошибок (confision matrix). \n",
        "\n",
        "**Полнота** (Sensitivity, True Positive Rate, Recall, Hit Rate) отражает какой процент объектов положительного класса мы правильно классифицировали.\n",
        "\n",
        "**Точность** (Precision, Positive Predictive Value) отражает какой процент положительных объектов (т.е. тех, что мы считаем положительными) правильно классифицирован. (Не путать с Accuracy!)\n",
        "\n",
        "Легко построить алгоритм со 100%-й полнотой: он все объекты относит к классу 1, но при этом точность может быть очень низкой. Нетрудно построить алгоритм с близкой к 100% точностью: он относит к классу 1 только те объекты, в которых уверен, при этом полнота может быть низкая.\n",
        "\n",
        "**F1-мера** (F1 score) является средним гармоническим точности и полноты, максимизация этого функционала приводит к одновременной максимизации этих двух «ортогональных критериев»\n",
        "\n",
        "$$F_1 = \\frac{2}{\\mathrm{recall}^{-1} + \\mathrm{precision}^{-1}} = 2 \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{\\mathrm{precision} + \\mathrm{recall}} = \\frac{\\mathrm{tp}}{\\mathrm{tp} + \\frac12 (\\mathrm{fp} + \\mathrm{fn}) } $$\n",
        "\n",
        "Также рассматривают весовое среднее гармоническое точности и полноты –  $F_\\beta$-меру:\n",
        "\n",
        "$$F_\\beta = (1 + \\beta^2) \\cdot \\frac{\\mathrm{precision} \\cdot \\mathrm{recall}}{(\\beta^2 \\cdot \\mathrm{precision}) + \\mathrm{recall}} = \\frac {(1 + \\beta^2) \\cdot \\mathrm{tp} }{(1 + \\beta^2) \\cdot \\mathrm{tp} + \\beta^2 \\cdot \\mathrm{fn} + \\mathrm{fp}}\\,$$\n",
        "\n",
        "Изменение $\\beta$ позволяет делать один из критериев (точность или полноту) важнее при оптимизации."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yiLk1P_xYQ2"
      },
      "source": [
        "## Биграммы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjy5ZPmwWg-j"
      },
      "source": [
        "Попробуем сделать то же самое, используя в качестве признаков униграммы и биграммы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKeS-Vmv13SE"
      },
      "source": [
        "# инициализируем векторайзер \n",
        "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmQHqUpRWg-k"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "bigram_vectorized_x_train = bigram_vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfG5x9i91_n4",
        "outputId": "f70e99b0-569f-4ae3-e38a-c0763225b26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(bigram_vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 509
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twUcp7eU2E-9"
      },
      "source": [
        "# применяем обученный векторизатор к тестовым данным\n",
        "bigram_vectorized_x_test = bigram_vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPsfMX7i2H1j",
        "outputId": "aa614932-3cf7-4833-f50d-e97698e31d86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(bigram_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.61      0.68      0.65      8985\n",
            "    negative       0.73      0.66      0.70      8998\n",
            "    positive       0.87      0.86      0.86      9018\n",
            "\n",
            "    accuracy                           0.73     27001\n",
            "   macro avg       0.74      0.73      0.73     27001\n",
            "weighted avg       0.74      0.73      0.74     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MonLW7AyWg-m"
      },
      "source": [
        "У меня получилось повысить точность на пару процентов по сравнению с униграммами"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdwrMRN93D31",
        "outputId": "17400e57-6144-437a-f920-4653a721bb58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "bigram_vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999, 464435)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 512
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHzVwaMF3LPY"
      },
      "source": [
        "\"Признаков\" объектов стало на порядок больше."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39SSh0zWg-r"
      },
      "source": [
        "## Токенизация\n",
        "\n",
        "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
        "\n",
        "Самый наивный способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoSe08N2Wg-r"
      },
      "source": [
        "import nltk # уже знакомая нам библиотека nltk\n",
        "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiDt3L8Y8god"
      },
      "source": [
        "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPH3yMcumsdd",
        "outputId": "e5026098-b86b-4d3f-a3f6-6836a46d7694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 514
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfDb8D_9DqD"
      },
      "source": [
        "Применим токенизацию:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrJDGpgYWg-4",
        "outputId": "f8316c34-6b76-46a3-96d0-85a56f513fbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence = 'Кто же победит на выборах в США: Трамп или Байден?'\n",
        "word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто',\n",
              " 'же',\n",
              " 'победит',\n",
              " 'на',\n",
              " 'выборах',\n",
              " 'в',\n",
              " 'США',\n",
              " ':',\n",
              " 'Трамп',\n",
              " 'или',\n",
              " 'Байден',\n",
              " '?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 515
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxy7KGZI9bhK"
      },
      "source": [
        "Сравните с использованием ```split()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p52dIuSI9W6o",
        "outputId": "720514f4-45ec-4bef-a4b0-843ba934ca4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "sentence.split()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Кто',\n",
              " 'же',\n",
              " 'победит',\n",
              " 'на',\n",
              " 'выборах',\n",
              " 'в',\n",
              " 'США:',\n",
              " 'Трамп',\n",
              " 'или',\n",
              " 'Байден?']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 516
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_702Dg5OWg-5"
      },
      "source": [
        "В nltk вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps8oPYoTWg-6",
        "outputId": "fe2c9cca-776d-4ae6-a00e-5fa56e1f6028",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordTokenizer',\n",
              " 'TweetTokenizer',\n",
              " 'WhitespaceTokenizer',\n",
              " 'WordPunctTokenizer']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 517
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnGCL5iWg-8"
      },
      "source": [
        "Одни умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jejj5X7QWg-8",
        "outputId": "f746e5c8-6798-4030-be59-cc4b384d5fc7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "list(wh_tok.span_tokenize(sentence))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 3),\n",
              " (4, 6),\n",
              " (7, 14),\n",
              " (15, 17),\n",
              " (18, 25),\n",
              " (26, 27),\n",
              " (28, 32),\n",
              " (33, 38),\n",
              " (39, 42),\n",
              " (43, 50)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 518
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-wf6A1EWg--"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2REwpHGWWg-_",
        "outputId": "ba839f04-108b-4d99-9b16-94e386634c0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 519
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tckre90JWg_B"
      },
      "source": [
        "А некоторые -- вообще не для текста на естественном языке:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F1Ml3xtaWg_D",
        "outputId": "23a1ed87-63f5-4698-d6b1-fd5098e1b6db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['(a (b c))', 'd', 'e', '(f)']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 520
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2kvAo0_b93"
      },
      "source": [
        "**Правильный токенизатор подбирается исходя из требований задачи!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhVrgkSaWg_K"
      },
      "source": [
        "## Стоп-слова\n",
        "\n",
        "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ld-h6WKyWg_K",
        "outputId": "6507dd0f-565c-4b9d-ef80-b4b32df34543",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# импортируем стоп-слова из библиотеки nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# посмотрим на стоп-слова для русского языка\n",
        "print(stopwords.words('russian'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yHg5Z93iAyee"
      },
      "source": [
        "noise = stopwords.words('russian')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3gweXaWWg_P"
      },
      "source": [
        "Теперь нужно обучать нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
        "\n",
        "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
        "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
        "* какой токенизатор мы используем, параметр **tokenizer**;\n",
        "* какие у нас стоп-слова, параметр **stop_words**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbXrVeRRuAxx"
      },
      "source": [
        "# инициализируем умный векторайзер \n",
        "smart_vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words=noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCM2Jy0448Mc"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTiC4oUX5T__",
        "outputId": "dd368d1f-a1e4-4574-90bd-cb9921f62a4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "smart_vectorized_x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(62999, 44738)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 525
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BztanE26o5Z",
        "outputId": "22882469-b18d-49bc-e793-2866f7e1cd61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list(smart_vectorizer.vocabulary_.items())[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('заказывала', 12609),\n",
              " ('31', 724),\n",
              " ('размер', 33742),\n",
              " ('пришел', 31678),\n",
              " ('кажется', 14854),\n",
              " ('25', 568),\n",
              " ('26', 582),\n",
              " ('очень', 25757),\n",
              " ('маленький', 18144),\n",
              " ('расстроило', 34400)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 526
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Nc6D-nwWg_P",
        "outputId": "2e8d574b-43e2-47bb-8687-0bd2630c3c96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(smart_vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 527
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ-P91PV5KST"
      },
      "source": [
        "# применяем обученный векторайзер к тестовым данным\n",
        "smart_vectorized_x_test = smart_vectorizer.transform(x_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QJ3elF85MDB",
        "outputId": "c17f1128-57b8-4b91-dd50-8ae40fef2fdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(smart_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.58      0.66      0.62      8985\n",
            "    negative       0.73      0.61      0.66      8998\n",
            "    positive       0.82      0.84      0.83      9018\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.71      0.71      0.71     27001\n",
            "weighted avg       0.71      0.71      0.71     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWB1foQWg_T"
      },
      "source": [
        "Получилось чуть хуже. \n",
        "\n",
        "Что ещё можно сделать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRf9T_SWg_U"
      },
      "source": [
        "## Лемматизация\n",
        "\n",
        "**Лемматизация** – это сведение разных форм одного слова к начальной форме – **лемме**. Почему это хорошо?\n",
        "* Во-первых, естественно рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть хороший лемматизатор pymorphy. \n",
        "\n",
        "Стемминг (англ. stemming — находить происхождение) — это процесс нахождения основы слова для заданного исходного слова. Основа слова не обязательно совпадает с морфологическим корнем слова. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKZG2MwWg_f"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcYWYq4BzOon",
        "outputId": "d87fba93-2fc8-4b61-80dc-447c5740f98c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# устанавливаем pymorphy2\n",
        "!pip install pymorphy2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.6/dist-packages (0.9.1)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqdT2pmRFn2F"
      },
      "source": [
        "В pymorphy2 для морфологического анализа слов есть ```MorphAnalyzer()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4nRuUu2Wg_g"
      },
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egd6KdqzWg_h"
      },
      "source": [
        "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6hdm1KBFx18"
      },
      "source": [
        "sentence = 'Кто же победит на выборах в США: Трамп или Байден?'\n",
        "sent = word_tokenize(sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr1C2beNF3vE"
      },
      "source": [
        "Лемматизируем слово \"победит\" из предложения ```sentence``` с помощью метода ```parse()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q3zNlPBWg_i",
        "outputId": "fb8c1e9f-cd4a-4815-def0-4e5efb811bae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "ana = pymorphy2_analyzer.parse(sent[2])\n",
        "ana"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parse(word='победит', tag=OpencorporaTag('VERB,perf,tran sing,3per,futr,indc'), normal_form='победить', score=0.846153, methods_stack=((DictionaryAnalyzer(), 'победит', 2483, 9),)),\n",
              " Parse(word='победит', tag=OpencorporaTag('NOUN,inan,masc sing,nomn'), normal_form='победит', score=0.076923, methods_stack=((DictionaryAnalyzer(), 'победит', 34, 0),)),\n",
              " Parse(word='победит', tag=OpencorporaTag('NOUN,inan,masc sing,accs'), normal_form='победит', score=0.076923, methods_stack=((DictionaryAnalyzer(), 'победит', 34, 3),))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 533
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2O2BL4_GJzq"
      },
      "source": [
        "Выведем его нормальную форму:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-zp0KZLWg_p",
        "outputId": "3b52efa3-80d0-43aa-82f5-2c1e616778ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ana[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'победить'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 534
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUzhGLTxLIr9"
      },
      "source": [
        "Нормализация предложения \"вижу три села\" может дать \"видеть тереть сесть\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldFCYxXuLL4p"
      },
      "source": [
        "sent2 = word_tokenize('вижу три села')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKRiNng-LfoP",
        "outputId": "58499c0a-c6e4-45b9-9ecf-6c8b0d0b3ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "ana2 = pymorphy2_analyzer.parse(sent2[2])\n",
        "ana2[0].normal_form"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'село'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 536
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWxW3e9Wg-m"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hCxZRtWg-m"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "**TF (term frequency)** – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_{t}}{\\sum_k n_{k}} $$\n",
        "\n",
        "**IDF (inverse document frequency)** – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\mbox{log} \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t, d, D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Cмысл: если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fv7DfTkJWg-n"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f_zZm14PHM"
      },
      "source": [
        "Действуем аналогично, как с ```CountVectorizer()```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JjF9m3EOQTBK"
      },
      "source": [
        "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VrTdCw_TDE9o"
      },
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPacf_DKDP7M",
        "outputId": "dd81b488-d089-4ca5-aa14-d4501bd06668",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(tfidf_vectorized_x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 540
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljAO8NIPDSnK",
        "outputId": "479dc7da-6925-4c96-9fac-1585c8a497c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# применяем обученный векторизатор к тестовым данным\n",
        "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.67      0.63      8985\n",
            "    negative       0.73      0.62      0.67      8998\n",
            "    positive       0.84      0.84      0.84      9018\n",
            "\n",
            "    accuracy                           0.71     27001\n",
            "   macro avg       0.72      0.71      0.71     27001\n",
            "weighted avg       0.72      0.71      0.71     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hedBdcYWhAH"
      },
      "source": [
        "Иногда пунктуация бывает и не шумом - главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию? На примере с твитами хорошо было бы видно, что пунктуация работает хорошо ))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhZMwsY5WhAI",
        "outputId": "2e794b03-3a5d-4c9d-eb17-0f5b796f6fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# инициализируем умный векторайзер stop-words НЕ ИСПОЛЬЗУЕМ!\n",
        "alternative_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
        "                                               tokenizer=word_tokenize)\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "alternative_tfidf_vectorized_x_train = alternative_tfidf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(alternative_tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "alternative_tfidf_vectorized_x_test = alternative_tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(alternative_tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.68      0.63      8985\n",
            "    negative       0.73      0.64      0.68      8998\n",
            "    positive       0.85      0.84      0.84      9018\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.72      0.72      0.72     27001\n",
            "weighted avg       0.72      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQbCtidWhAP"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhUG9qWuWhAQ",
        "outputId": "04302da9-fb1b-44d0-891e-9ad74c87540e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "cool_token = 'плохо'\n",
        "pred = ['positive' if cool_token in review else 'negative' for review in x_test]\n",
        "print(classification_report(pred, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.00      0.00      0.00         0\n",
            "    negative       0.94      0.33      0.49     25787\n",
            "    positive       0.02      0.14      0.03      1214\n",
            "\n",
            "    accuracy                           0.32     27001\n",
            "   macro avg       0.32      0.15      0.17     27001\n",
            "weighted avg       0.90      0.32      0.47     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrqW55jgWhAR"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "В некоторых задачах в качестве признаков могут быть использщованы, n-граммы символов. Для этого необходимо установить в ```CountVectorizer()``` параметр ```analyzer = 'char'```, то есть анализировать символы."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4lNhEmyWhAU",
        "outputId": "3e44c09e-d140-4cc5-b031-bec65c6008a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# инициализируем векторайзер для символов\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 6))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "char_vectorized_x_train = char_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = MultinomialNB()\n",
        "clf.fit(char_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "char_vectorized_x_test = char_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(char_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    neautral       0.59      0.71      0.65      8985\n",
            "    negative       0.74      0.62      0.67      8998\n",
            "    positive       0.87      0.83      0.85      9018\n",
            "\n",
            "    accuracy                           0.72     27001\n",
            "   macro avg       0.73      0.72      0.72     27001\n",
            "weighted avg       0.73      0.72      0.72     27001\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLMMicsFWhAY"
      },
      "source": [
        "Cимвольные n-граммы используются, например, для задачи определения языка. Ещё одна замечательная особенность признаков-символов - для них не нужна токенизация и лемматизация, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJox-LoonoPx"
      },
      "source": [
        "## Задание 5\n",
        "\n",
        "Применим полученные выше навыки и решим задачу анализа тональности отзывов. (Те, кто предпочитает работать с английским языком, могут использовать набор данных `sms_spam`, он есть в папке `Data`).\n",
        "\n",
        "Нужно повторить весь пайплайн от сырых текстов до получения обученной модели.\n",
        "\n",
        "Обязательные шаги предобработки:\n",
        "1. токенизация\n",
        "2. приведение к нижнему регистру\n",
        "3. удаление стоп-слов\n",
        "4. лемматизация\n",
        "5. векторизация (с настройкой гиперпараметров)\n",
        "6. построение модели\n",
        "7. оценка качества модели\n",
        "\n",
        "Обязательно использование векторайзеров:\n",
        "1. мешок n-грамм (диапазон для n подбирайте самостоятельно, запрещено использовать только униграммы).\n",
        "2. tf-idf ((диапазон для n подбирайте самостоятельно, также нужно подбирать параметры max_df, min_df, max_features)\n",
        "3. символьные n-граммы (диапазон для n подбирайте самостоятельно)\n",
        "\n",
        "В качестве классификатора нужно использовать наивный байесовский классификатор. \n",
        "\n",
        "Для сравнения векторайзеров между собой используйте precision, recall, f1-score и accuracy. Для этого сформируйте датафрейм, в котором в строках будут разные векторайзеры, а в столбцах разные метрики качества, а в  ячейках будут значения этих метрик для соответсвующих векторайзеров."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTXlJLHGDrnT"
      },
      "source": [
        "Получим список предложений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFKfTGqQS4UY",
        "outputId": "4bd339ca-c5ca-457b-dfb0-892328276dbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89995</th>\n",
              "      <td>сделано достаточно хорошо. на ткани сделан рис...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89996</th>\n",
              "      <td>Накидка шикарная. Спасибо большое провдо линяе...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89997</th>\n",
              "      <td>спасибо большое ) продовца рекомендую.. заказа...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89998</th>\n",
              "      <td>Очень довольна заказом! Меньше месяца в РБ.  К...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>89999</th>\n",
              "      <td>хорошая куртка. постороннего запаха нет. швы р...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>90000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  review sentiment\n",
              "0      качество плохое пошив ужасный (горловина напер...  negative\n",
              "1      Товар отдали другому человеку, я не получила п...  negative\n",
              "2      Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
              "3      товар не пришел, продавец продлил защиту без м...  negative\n",
              "4          Кофточка голая синтетика, носить не возможно.  negative\n",
              "...                                                  ...       ...\n",
              "89995  сделано достаточно хорошо. на ткани сделан рис...  positive\n",
              "89996  Накидка шикарная. Спасибо большое провдо линяе...  positive\n",
              "89997  спасибо большое ) продовца рекомендую.. заказа...  positive\n",
              "89998  Очень довольна заказом! Меньше месяца в РБ.  К...  positive\n",
              "89999  хорошая куртка. постороннего запаха нет. швы р...  positive\n",
              "\n",
              "[90000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 545
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iUaBZqFv78O_"
      },
      "source": [
        "sentences  = list(data.iloc[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ4kYGiI4E-j"
      },
      "source": [
        "### (уберем знаки перпинания)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV6KJdDS726s"
      },
      "source": [
        "for ind_sentence in range(len(sentences)):\n",
        "  for ch in string.punctuation:\n",
        "    sentences[ind_sentence] = sentences[ind_sentence].replace(ch , \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_loyuqpbEHnv"
      },
      "source": [
        "###  2. приведение к нижнему регистру"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F34N_2rD4OY"
      },
      "source": [
        "sentences = [sentence.lower() for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEVbAzZLFIxO"
      },
      "source": [
        "### 1. токенизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bPuL40etEGpz"
      },
      "source": [
        "token_sentences = [word_tokenize(sentence) for sentence in sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdx0NV9IXH04",
        "outputId": "4302682e-52c7-4a48-849e-a24ce86587f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "token_sentences[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['качество',\n",
              "  'плохое',\n",
              "  'пошив',\n",
              "  'ужасный',\n",
              "  'горловина',\n",
              "  'наперекос',\n",
              "  'фото',\n",
              "  'не',\n",
              "  'соответствует',\n",
              "  'ткань',\n",
              "  'ужасная',\n",
              "  'рисунок',\n",
              "  'блеклый',\n",
              "  'маленький',\n",
              "  'рукав',\n",
              "  'не',\n",
              "  'такой',\n",
              "  'ужас',\n",
              "  'не',\n",
              "  'стоит',\n",
              "  'за',\n",
              "  'такие',\n",
              "  'деньги',\n",
              "  'г'],\n",
              " ['товар',\n",
              "  'отдали',\n",
              "  'другому',\n",
              "  'человеку',\n",
              "  'я',\n",
              "  'не',\n",
              "  'получила',\n",
              "  'посылку',\n",
              "  'ладно',\n",
              "  'хоть',\n",
              "  'деньги',\n",
              "  'вернули'],\n",
              " ['ужасная',\n",
              "  'синтетика',\n",
              "  'тонкая',\n",
              "  'ничего',\n",
              "  'общего',\n",
              "  'с',\n",
              "  'представленной',\n",
              "  'картинкой',\n",
              "  'не',\n",
              "  'яркая',\n",
              "  'рисунок',\n",
              "  'растянут',\n",
              "  'и',\n",
              "  'тусклый',\n",
              "  'впрочем',\n",
              "  'как',\n",
              "  'и',\n",
              "  'сама',\n",
              "  'кофта',\n",
              "  'мешок',\n",
              "  'на',\n",
              "  'картинке',\n",
              "  'кажется',\n",
              "  'приталенной',\n",
              "  'на',\n",
              "  'самом',\n",
              "  'деле',\n",
              "  'нет',\n",
              "  'не',\n",
              "  'рекомендую'],\n",
              " ['товар',\n",
              "  'не',\n",
              "  'пришел',\n",
              "  'продавец',\n",
              "  'продлил',\n",
              "  'защиту',\n",
              "  'без',\n",
              "  'моего',\n",
              "  'согласия',\n",
              "  'от',\n",
              "  'продавца',\n",
              "  'одни',\n",
              "  'обещания'],\n",
              " ['кофточка', 'голая', 'синтетика', 'носить', 'не', 'возможно']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 550
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMjd3jwn41PS"
      },
      "source": [
        "###  3. удаление стоп-слов\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbsHicmZ4-lF",
        "outputId": "3e072194-1d94-43fd-c957-0af76aab4956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "stop_words = stopwords.words('russian')\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['и',\n",
              " 'в',\n",
              " 'во',\n",
              " 'не',\n",
              " 'что',\n",
              " 'он',\n",
              " 'на',\n",
              " 'я',\n",
              " 'с',\n",
              " 'со',\n",
              " 'как',\n",
              " 'а',\n",
              " 'то',\n",
              " 'все',\n",
              " 'она',\n",
              " 'так',\n",
              " 'его',\n",
              " 'но',\n",
              " 'да',\n",
              " 'ты',\n",
              " 'к',\n",
              " 'у',\n",
              " 'же',\n",
              " 'вы',\n",
              " 'за',\n",
              " 'бы',\n",
              " 'по',\n",
              " 'только',\n",
              " 'ее',\n",
              " 'мне',\n",
              " 'было',\n",
              " 'вот',\n",
              " 'от',\n",
              " 'меня',\n",
              " 'еще',\n",
              " 'нет',\n",
              " 'о',\n",
              " 'из',\n",
              " 'ему',\n",
              " 'теперь',\n",
              " 'когда',\n",
              " 'даже',\n",
              " 'ну',\n",
              " 'вдруг',\n",
              " 'ли',\n",
              " 'если',\n",
              " 'уже',\n",
              " 'или',\n",
              " 'ни',\n",
              " 'быть',\n",
              " 'был',\n",
              " 'него',\n",
              " 'до',\n",
              " 'вас',\n",
              " 'нибудь',\n",
              " 'опять',\n",
              " 'уж',\n",
              " 'вам',\n",
              " 'ведь',\n",
              " 'там',\n",
              " 'потом',\n",
              " 'себя',\n",
              " 'ничего',\n",
              " 'ей',\n",
              " 'может',\n",
              " 'они',\n",
              " 'тут',\n",
              " 'где',\n",
              " 'есть',\n",
              " 'надо',\n",
              " 'ней',\n",
              " 'для',\n",
              " 'мы',\n",
              " 'тебя',\n",
              " 'их',\n",
              " 'чем',\n",
              " 'была',\n",
              " 'сам',\n",
              " 'чтоб',\n",
              " 'без',\n",
              " 'будто',\n",
              " 'чего',\n",
              " 'раз',\n",
              " 'тоже',\n",
              " 'себе',\n",
              " 'под',\n",
              " 'будет',\n",
              " 'ж',\n",
              " 'тогда',\n",
              " 'кто',\n",
              " 'этот',\n",
              " 'того',\n",
              " 'потому',\n",
              " 'этого',\n",
              " 'какой',\n",
              " 'совсем',\n",
              " 'ним',\n",
              " 'здесь',\n",
              " 'этом',\n",
              " 'один',\n",
              " 'почти',\n",
              " 'мой',\n",
              " 'тем',\n",
              " 'чтобы',\n",
              " 'нее',\n",
              " 'сейчас',\n",
              " 'были',\n",
              " 'куда',\n",
              " 'зачем',\n",
              " 'всех',\n",
              " 'никогда',\n",
              " 'можно',\n",
              " 'при',\n",
              " 'наконец',\n",
              " 'два',\n",
              " 'об',\n",
              " 'другой',\n",
              " 'хоть',\n",
              " 'после',\n",
              " 'над',\n",
              " 'больше',\n",
              " 'тот',\n",
              " 'через',\n",
              " 'эти',\n",
              " 'нас',\n",
              " 'про',\n",
              " 'всего',\n",
              " 'них',\n",
              " 'какая',\n",
              " 'много',\n",
              " 'разве',\n",
              " 'три',\n",
              " 'эту',\n",
              " 'моя',\n",
              " 'впрочем',\n",
              " 'хорошо',\n",
              " 'свою',\n",
              " 'этой',\n",
              " 'перед',\n",
              " 'иногда',\n",
              " 'лучше',\n",
              " 'чуть',\n",
              " 'том',\n",
              " 'нельзя',\n",
              " 'такой',\n",
              " 'им',\n",
              " 'более',\n",
              " 'всегда',\n",
              " 'конечно',\n",
              " 'всю',\n",
              " 'между']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 551
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L40QHyhJ5G6D",
        "outputId": "d59808ac-246e-440b-aba7-32a206c85544",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "token_sentences = [[word for word in sentence if word not in stop_words] for sentence in token_sentences]\n",
        "token_sentences[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['качество',\n",
              "  'плохое',\n",
              "  'пошив',\n",
              "  'ужасный',\n",
              "  'горловина',\n",
              "  'наперекос',\n",
              "  'фото',\n",
              "  'соответствует',\n",
              "  'ткань',\n",
              "  'ужасная',\n",
              "  'рисунок',\n",
              "  'блеклый',\n",
              "  'маленький',\n",
              "  'рукав',\n",
              "  'ужас',\n",
              "  'стоит',\n",
              "  'такие',\n",
              "  'деньги',\n",
              "  'г'],\n",
              " ['товар',\n",
              "  'отдали',\n",
              "  'другому',\n",
              "  'человеку',\n",
              "  'получила',\n",
              "  'посылку',\n",
              "  'ладно',\n",
              "  'деньги',\n",
              "  'вернули'],\n",
              " ['ужасная',\n",
              "  'синтетика',\n",
              "  'тонкая',\n",
              "  'общего',\n",
              "  'представленной',\n",
              "  'картинкой',\n",
              "  'яркая',\n",
              "  'рисунок',\n",
              "  'растянут',\n",
              "  'тусклый',\n",
              "  'сама',\n",
              "  'кофта',\n",
              "  'мешок',\n",
              "  'картинке',\n",
              "  'кажется',\n",
              "  'приталенной',\n",
              "  'самом',\n",
              "  'деле',\n",
              "  'рекомендую'],\n",
              " ['товар',\n",
              "  'пришел',\n",
              "  'продавец',\n",
              "  'продлил',\n",
              "  'защиту',\n",
              "  'моего',\n",
              "  'согласия',\n",
              "  'продавца',\n",
              "  'одни',\n",
              "  'обещания'],\n",
              " ['кофточка', 'голая', 'синтетика', 'носить', 'возможно']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 552
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuQbimfALGv9"
      },
      "source": [
        "### 4. лемматизация"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he_EewLnLI5U"
      },
      "source": [
        "for ind_sentence in range(len(token_sentences)):\n",
        "  for ind_word in range(len(token_sentences[ind_sentence])):\n",
        "    morpth_words = pymorphy2_analyzer.parse(token_sentences[ind_sentence][ind_word])\n",
        "    token_sentences[ind_sentence][ind_word] = morpth_words[0].normal_form"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VdfxCLzQtxb",
        "outputId": "0a4e0c5d-5562-4874-e30f-cd4c9102c62f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "token_sentences[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['качество',\n",
              "  'плохой',\n",
              "  'пошив',\n",
              "  'ужасный',\n",
              "  'горловина',\n",
              "  'наперекос',\n",
              "  'фото',\n",
              "  'соответствовать',\n",
              "  'ткань',\n",
              "  'ужасный',\n",
              "  'рисунок',\n",
              "  'блёклый',\n",
              "  'маленький',\n",
              "  'рукав',\n",
              "  'ужас',\n",
              "  'стоить',\n",
              "  'такой',\n",
              "  'деньга',\n",
              "  'г'],\n",
              " ['товар',\n",
              "  'отдать',\n",
              "  'другой',\n",
              "  'человек',\n",
              "  'получить',\n",
              "  'посылка',\n",
              "  'ладный',\n",
              "  'деньга',\n",
              "  'вернуть'],\n",
              " ['ужасный',\n",
              "  'синтетик',\n",
              "  'тонкий',\n",
              "  'общий',\n",
              "  'представить',\n",
              "  'картинка',\n",
              "  'яркий',\n",
              "  'рисунок',\n",
              "  'растянутый',\n",
              "  'тусклый',\n",
              "  'сам',\n",
              "  'кофта',\n",
              "  'мешок',\n",
              "  'картинка',\n",
              "  'казаться',\n",
              "  'приталенный',\n",
              "  'сам',\n",
              "  'дело',\n",
              "  'рекомендовать'],\n",
              " ['товар',\n",
              "  'прийти',\n",
              "  'продавец',\n",
              "  'продлить',\n",
              "  'защита',\n",
              "  'мой',\n",
              "  'согласие',\n",
              "  'продавец',\n",
              "  'один',\n",
              "  'обещание'],\n",
              " ['кофточка', 'голый', 'синтетик', 'носить', 'возможно']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 554
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23jGw1l3eaOH"
      },
      "source": [
        "### 5. векторизация (с настройкой гиперпараметров)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_W97xLbQf2ps"
      },
      "source": [
        "token_sentences = [' '.join(sentence) for sentence in token_sentences]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uV-cJKubfcUA"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(token_sentences, data.sentiment, train_size = 0.7)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUQJ6FHI_QgK"
      },
      "source": [
        "мешок n-грамм"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BVwxGhhTefwz"
      },
      "source": [
        "vectorizer_bag = CountVectorizer(ngram_range=(1, 3))\n",
        "vectorizer_bag_sentences_train = vectorizer_bag.fit_transform(x_train)\n",
        "vectorizer_bag_sentences_test = vectorizer_bag.transform(x_test)\n",
        "vectorizer_bag_sentences_train\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectorizer_bag_sentences_train, y_train)\n",
        "pred = clf.predict(vectorizer_bag_sentences_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-J1-kg27Kkl"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "ps = precision_score(pred, y_test, average='micro')\n",
        "rs = recall_score(pred, y_test, average='micro')\n",
        "fs = f1_score(pred, y_test, average='micro')\n",
        "acs = accuracy_score(pred, y_test)\n",
        "metric_frame = pd.DataFrame([['Bag', ps, rs, fs, acs]], \n",
        "                            columns = ['vectorizer', 'precision_score', 'recall_score', \n",
        "                                        'f1_score', 'accuracy_score']).set_index('vectorizer', drop=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9ZivdQLiL4S"
      },
      "source": [
        "tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYyf3eT1jTyn"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3), min_df=10, max_df=5000, max_features=20000)\n",
        "vectorizer_tfidf_sentences_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "vectorizer_tfidf_sentences_test = tfidf_vectorizer.transform(x_test)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectorizer_tfidf_sentences_train, y_train)\n",
        "pred = clf.predict(vectorizer_tfidf_sentences_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XnMCgOakEko"
      },
      "source": [
        "ps = precision_score(pred, y_test, average='micro')\n",
        "rs = recall_score(pred, y_test, average='micro')\n",
        "fs = f1_score(pred, y_test, average='micro')\n",
        "acs = accuracy_score(pred, y_test)\n",
        "metric_frame.loc['tfidf'] = [ps, rs, fs, acs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6iVgiFokpaI"
      },
      "source": [
        "символьные n-граммы"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVBjTUJfk3Hx"
      },
      "source": [
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(3, 8))\n",
        "vectorizer_char_sentences_train = char_vectorizer.fit_transform(x_train)\n",
        "vectorizer_char_sentences_test = char_vectorizer.transform(x_test)\n",
        "clf = MultinomialNB()\n",
        "clf.fit(vectorizer_char_sentences_train, y_train)\n",
        "pred = clf.predict(vectorizer_char_sentences_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7cPMkcFlUH6"
      },
      "source": [
        "ps = precision_score(pred, y_test, average='micro')\n",
        "rs = recall_score(pred, y_test, average='micro')\n",
        "fs = f1_score(pred, y_test, average='micro')\n",
        "acs = accuracy_score(pred, y_test)\n",
        "metric_frame.loc['char'] = [ps, rs, fs, acs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28N-_1m8BkK2"
      },
      "source": [
        "оценка качества модели(ей)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miBKdipDBS_c",
        "outputId": "8ef48ee1-28ed-4e82-ac93-a0bc93f03e84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "metric_frame"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>f1_score</th>\n",
              "      <th>accuracy_score</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>vectorizer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Bag</th>\n",
              "      <td>0.710418</td>\n",
              "      <td>0.710418</td>\n",
              "      <td>0.710418</td>\n",
              "      <td>0.710418</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>tfidf</th>\n",
              "      <td>0.706159</td>\n",
              "      <td>0.706159</td>\n",
              "      <td>0.706159</td>\n",
              "      <td>0.706159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>char</th>\n",
              "      <td>0.707826</td>\n",
              "      <td>0.707826</td>\n",
              "      <td>0.707826</td>\n",
              "      <td>0.707826</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            precision_score  recall_score  f1_score  accuracy_score\n",
              "vectorizer                                                         \n",
              "Bag                0.710418      0.710418  0.710418        0.710418\n",
              "tfidf              0.706159      0.706159  0.706159        0.706159\n",
              "char               0.707826      0.707826  0.707826        0.707826"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 563
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Бонус 1. Регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "source": [
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2athHzKuWhAd",
        "outputId": "2edb3091-4505-49d3-c768-cfc3a78d99cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['abcd', 'abca']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZR2AEq3WhAg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVKdRoc1WhAl",
        "outputId": "8bbfd2f6-cf88-4d46-c4d3-73cd5591c877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie') \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie', ' weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9U9EQZMwWhAn",
        "outputId": "f69ab957-340c-4e20-b2bb-ed3dc1ac6b5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVgPSjEOWhAp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az3KxKWwWhAr",
        "outputId": "8ba5d031-3396-4629-f213-be4e326f2326",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "result = re.sub('a', 'b', 'abcabc')\n",
        "print (result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_Sdu7xlWhAu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwNS9zt4WhAv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JstTupisWhAy",
        "outputId": "fde28971-91c9-4488-f7f9-0f59e24309ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 388
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFvnIWbUWhA2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZ5qn3DWhA3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPgOkJXhZxJ-"
      },
      "source": [
        "## Бонус 2: Word2Vec\n",
        "\n",
        "Векторные модели, которые мы рассматривали до этого (BOW, мешок слов; TF-IDF), условно называются *счётными*. Они основываются на том, что так или иначе \"считают\" слова и их соседей, и на основе этого строят вектора для слов. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL7VqGHwZmUG"
      },
      "source": [
        "\n",
        "Другой класс моделей, который повсеместно распространён на сегодняшний день, называется *предсказательными* моделями. Идея этих моделей заключается в использовании нейросетевых архитектур, которые \"предсказывают\" (а не считают) соседей для каждого слова.\n",
        "\n",
        "Одной из самых известных таких моделей является `word2vec`. Технология основана на нейронной сети, предсказывающей вероятность встретить слово в заданном контексте. Этот инструмент был разработан группой исследователей Google в 2013 году, руководителем проекта был Томаш Миколов (сейчас работает в Facebook). Вот две самые главные статьи:\n",
        "\n",
        "* [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)\n",
        "* [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
        "\n",
        "\n",
        "Полученные таким образом вектора называются *распределенными представлениями слов*, вложениями или **эмбеддингами**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V7k_qL1Z-UG"
      },
      "source": [
        "### Как это обучается?\n",
        "Мы задаём вектор для каждого слова с помощью матрицы $w$ и вектор контекста с помощью матрицы $W$. По сути, word2vec является обобщающим названием для двух архитектур Skip-Gram и Continuous Bag-Of-Words (CBOW).  \n",
        "\n",
        "![](https://www.researchgate.net/profile/Daniel_Braun6/publication/326588219/figure/fig1/AS:652185784295425@1532504616288/Continuous-Bag-of-words-CBOW-CB-and-Skip-gram-SG-training-model-illustrations.png)\n",
        "\n",
        "**CBOW** предсказывает текущее слово, исходя из окружающего его контекста. \n",
        "\n",
        "**Skip-gram**, наоборот, использует текущее слово, чтобы предугадывать окружающие его слова. \n",
        "\n",
        "### Как это работает?\n",
        "Word2vec принимает большой текстовый корпус в качестве входных данных и сопоставляет каждому слову вектор, выдавая координаты слов на выходе. Сначала он создает словарь, «обучаясь» на входных текстовых данных, а затем вычисляет векторное представление слов. \n",
        "\n",
        "Векторное представление основывается на *контекстной близости*: слова, встречающиеся в тексте рядом с одинаковыми словами (а следовательно, согласно дистрибутивной гипотезе, имеющие схожий смысл), в векторном представлении будут иметь близкие координаты векторов-слов. Для вычисления близости слов используется косинусное расстояние между их векторами.\n",
        "\n",
        "\n",
        "С помощью дистрибутивных векторных моделей можно строить семантические пропорции (они же аналогии: А относится к B так же, как C относится к D) и решать примеры:\n",
        "\n",
        "* *король: мужчина = королева: женщина* \n",
        " $\\Rightarrow$ \n",
        "* *король - мужчина + женщина = королева*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l0MPc-3NaOoE"
      },
      "source": [
        "![w2v](https://cdn-images-1.medium.com/max/2600/1*sXNXYfAqfLUeiDXPCo130w.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29tswg7NalHk"
      },
      "source": [
        "Для слов нет лейблов, но мы используем слова в контексте для сбора обучающей выборки.\n",
        "\n",
        "## Skip gram\n",
        "\n",
        "(Предсказание контекста по слову, один из основных параметров - windows_size)\n",
        "\n",
        "![Замещающий текст](http://mccormickml.com/assets/word2vec/training_data.png)\n",
        "\n",
        "1. Представляем корпус текста в формате One-hot encoding, подаем вектор на вход нейросети\n",
        "\n",
        "2. В качестве активации последнего слоя используем softmax -> переходим в пространство вероятностей (как будто задача классификации с очень большим количеством классов)\n",
        "\n",
        "3. Предсказываем слово контекста по максимальной вероятности\n",
        "\n",
        "![Замещающий текст](https://miro.medium.com/max/875/0*FD_ZSVKFywSg-CJM.png)\n",
        "\n",
        "Модель хорошо работает с небольшим количеством тренировочных данных\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTVRguH2Ryef"
      },
      "source": [
        "## CBOW\n",
        "\n",
        "![](https://iksinc.files.wordpress.com/2015/04/screen-shot-2015-04-12-at-10-58-21-pm.png)\n",
        "\n",
        "Тренируется быстрее, чем SkipGram, лучше точность на редких словах\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7KSbkvwa76d"
      },
      "source": [
        "### Проблемы\n",
        "Невозможно установить тип семантических отношений между словами: синонимы, антонимы и т.д. будут одинаково близки, потому что обычно употребляются в схожих контекстах. Поэтому близкие в векторном пространстве слова называют *семантическими ассоциатами*. Это значит, что они семантически связаны, но как именно — непонятно.\n",
        "\n",
        "\n",
        "## RusVectōrēs\n",
        "\n",
        "\n",
        "На сайте [RusVectōrēs](https://rusvectores.org/ru/) собраны предобученные на различных данных модели для русского языка, а также можно поискать наиболее близкие слова к заданному, посчитать семантическую близость нескольких слов и порешать примеры с помощью «калькулятора семантической близости».\n",
        "\n",
        "\n",
        "Для других языков также можно найти предобученные модели — например, модели [fastText](https://fasttext.cc/docs/en/english-vectors.html) и [GloVe](https://nlp.stanford.edu/projects/glove/)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwcFQkV7bA0M"
      },
      "source": [
        "## Gensim\n",
        "\n",
        "Использовать предобученную модель эмбеддингов или обучить свою можно с помощью библиотеки `gensim`. Вот [ее документация](https://radimrehurek.com/gensim/models/word2vec.html).\n",
        "\n",
        "### Как использовать готовую модель\n",
        "\n",
        "Модели word2vec бывают разных форматов:\n",
        "\n",
        "* .vec.gz — обычный файл (текстовый)\n",
        "* .bin.gz — бинарный файл\n",
        "\n",
        "Загружаются они с помощью одного и того же класса `KeyedVectors`, меняется только параметр `binary` у функции `load_word2vec_format`. \n",
        "\n",
        "Если же эмбеддинги обучены **не** с помощью word2vec, то для загрузки нужно использовать функцию `load`. Т.е. **для загрузки предобученных эмбеддингов *glove, fasttext, bpe* и любых других нужна именно она**.\n",
        "\n",
        "Скачаем с RusVectōrēs модель для русского языка, обученную на НКРЯ образца 2015 г. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTnxWXhubJ3W"
      },
      "source": [
        "import urllib.request # библиотека для скачивания данных\n",
        "import gensim # библиотека для загрузки и использвоания моделй w2v\n",
        "from gensim.models import word2vec # непосредственно методы w2v"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhY2nOyTbM1h",
        "outputId": "2f6d2100-6292-410b-c7a4-d4e41fcfbd2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        }
      },
      "source": [
        "# скачиваем модель ruscorpora_mystem_cbow_300 с сайта rusvectores\n",
        "# 300 - размерность вектора embeddings для слов\n",
        "\n",
        "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-390-4e7097fa5922>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# 300 - размерность вектора embeddings для слов\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ruscorpora_mystem_cbow_300_2_2015.bin.gz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/lib/python3.6/urllib/request.py\u001b[0m in \u001b[0;36murlretrieve\u001b[0;34m(url, filename, reporthook, data)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m                 \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0;31m# Amount is given, implement using readinto\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m             \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m         \u001b[0;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 507\u001b[0;31m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIY8HdoEbRCE"
      },
      "source": [
        "Загружаем скачанную модель. Обратите внимание, что мы скачали бинарный файл (.bin.gz), поэтому у функции load_word2vec_format() параметр binary=True"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0YvC5bBbVVm"
      },
      "source": [
        "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'\n",
        "\n",
        "model_ru = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udp_Onx4bX3D"
      },
      "source": [
        "Посмотрим на ближайших соседей следующей группы слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WoLtzabZ57"
      },
      "source": [
        "words = ['день_S', 'ночь_S', 'человек_S', 'семантика_S', 'биткоин_S']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1wix4nwbfLf"
      },
      "source": [
        "Частеречные тэги (например, _S, тег части речи слова) нужны, поскольку это специфика скачанной модели - она была натренирована на словах, размеченных по частям речи (и лемматизированных). \n",
        "\n",
        "**NB!** В названиях моделей на `rusvectores` указано, какой тегсет (набор обозначений тегов) они используют (mystem, upos и т.д.)\n",
        "\n",
        "Попросим у модели 10 ближайших соседей для каждого слова и косинусные близости для каждого:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xIsRf1Ebhrd"
      },
      "source": [
        "for word in words:\n",
        "    # есть ли слово в модели? \n",
        "    if word in model_ru:\n",
        "        print(word)\n",
        "        # смотрим на вектор слова (его размерность 300, смотрим на первые 10 чисел)\n",
        "        print(model_ru[word][:10])\n",
        "        # выдаем 10 ближайших соседей слова:\n",
        "        for word, sim in model_ru.most_similar(positive=[word], topn=10):\n",
        "            # слово + коэффициент косинусной близости\n",
        "            print(word, ': ', sim)\n",
        "        print('\\n')\n",
        "    else:\n",
        "        # Увы!\n",
        "        print('Увы, слова \"%s\" нет в модели!' % word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSKvti4ObmKP"
      },
      "source": [
        "Найдем похожесть пары слов функцией ```similarity()``` (там используется косинусная мера схожести):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dbBhOf3bvZj"
      },
      "source": [
        "print(model_ru.similarity('человек_S', 'обезьяна_S'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2vjNfsXb1bF"
      },
      "source": [
        "У загруженной модели много различных функций. Например, можно решать задачи на семантическую близость.\n",
        "\n",
        "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?\n",
        "\n",
        "Для решения примера в качестве параметров метода ```most_similar()``` необходимо передать:\n",
        "* positive — вектора, которые мы складываем\n",
        "* negative — вектора, которые вычитаем\n",
        "\n",
        "*Замечание:* не забываем взять самый близкий элемент, для этого необходимо указать ```[0][0]```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HE75DAGb40E"
      },
      "source": [
        "Что получится, если вычесть из пиццы Италию и прибавить Сибирь?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nz81U7aKb8KD"
      },
      "source": [
        "print(model_ru.most_similar(negative=[ 'италия_S'], positive=['пицца_S','сибирь_S'])[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWRxadscb-oY"
      },
      "source": [
        "print(model_ru.most_similar(positive=['футбол_S', 'хоккей_S'], negative=['россия_S'])[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfmyC1sQcAme"
      },
      "source": [
        "**Задание.** Придумайте и проверьте с помощью метода `most_similar` несколько аналогий"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apY4f9DHcIn7"
      },
      "source": [
        "Метод ```doesnt_match()``` находит \"лишнее слово\" в группе слов:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoHVj3XscPTf"
      },
      "source": [
        "model_ru.doesnt_match('пицца_S пельмень_S хот-дог_S ананас_S'.split())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz5cyw-ScRFd"
      },
      "source": [
        "**Задание.** Придумайте и проверьте с помощью метода `doesnt_match` несколько последовательностей с лишними словами"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16c7mAZ_kd68"
      },
      "source": [
        "### Как обучить свою модель\n",
        "\n",
        "В качестве обучающих данных возьмем размеченные и неразмеченные отзывы о фильмах (датасет взят с Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EX66ryM0klX5"
      },
      "source": [
        "# скачиваем датасет\n",
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/unlabeledTrainData.tsv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-gLAXFcvkiOM"
      },
      "source": [
        "Загрузим датасет в датафрейм и посмотрим на него, делаем это с помощью уже привычной библиотеки **pandas**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okHBivCVksUW"
      },
      "source": [
        "# считываем данные в формате csv\n",
        "data = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\", quoting=3)\n",
        "\n",
        "len(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHdjpqcCkyT1"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "im53Mp3yk1ld"
      },
      "source": [
        "data.iloc[10]['review']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knptEXQtkhW7"
      },
      "source": [
        "Нам необходимо отчистить данные от лишнего: убрать ссылки, html-разметку и небуквенные символы. Затем нужно привести все к нижнему регистру и токенизировать. \n",
        "\n",
        "На выходе мы хотим получить массив из предложений, каждое из которых представляет собой массив слов.\n",
        "\n",
        "Импортируем необходимые библиотеки и методы (некоторые уже были испортированы ранее, но для полноты картины оставим их):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0mEEpUmk6J6"
      },
      "source": [
        "import nltk.data # библиотека Natural Language Toolkit\n",
        "import re   # библиотека для регулярных выражений\n",
        "from bs4 import BeautifulSoup # библиотека для парсинга xml\n",
        "from nltk.corpus import stopwords # стоп-слова из NLTK\n",
        "from nltk.tokenize import sent_tokenize, RegexpTokenizer  # токенизаторы из NLTK\n",
        "nltk.download('punkt') # пунктуация для правильной работы токенизатора"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nhz3Olb4lDvz"
      },
      "source": [
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yhts6MulJaD"
      },
      "source": [
        "Функции для очистки данных:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_veEBUBZlKN6"
      },
      "source": [
        "def review_to_wordlist(review, remove_stopwords=False):\n",
        "    # убираем ссылки вне тегов\n",
        "    review = re.sub(r\"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\", \" \", review)\n",
        "    # достаем сам текст\n",
        "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
        "    # оставляем только буквенные символы\n",
        "    review_text = re.sub(\"[^a-zA-Z]\",\" \", review_text)\n",
        "    # приводим к нижнему регистру и разбиваем на слова по символу пробела\n",
        "    words = review_text.lower().split()\n",
        "    if remove_stopwords:\n",
        "      # убираем стоп-слова\n",
        "        stops = stopwords.words(\"english\")\n",
        "        words = [w for w in words if not w in stops]\n",
        "    return(words)\n",
        "\n",
        "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
        "    return sentences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfEIEFIJlTBH"
      },
      "source": [
        "Проходим по всему датасету и парсим написанной выше функцией  текст в списки слов, удаляя при этом лишнее:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ujigc0V2lWmw"
      },
      "source": [
        "sentences = []  \n",
        "\n",
        "print(\"Parsing sentences from training set...\")\n",
        "for review in data[\"review\"]:\n",
        "    sentences += review_to_sentences(review, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYePh1_GlVH5"
      },
      "source": [
        "Посмотрим, что получилось:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYsO6Ya7lanY"
      },
      "source": [
        "print(len(sentences))\n",
        "print(sentences[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNp27TyyljLr"
      },
      "source": [
        "# это понадобится нам позже для обучения другой модели эмбеддингов \n",
        "\n",
        "with open('clean_text.txt', 'w') as f:\n",
        "    for s in sentences[:5000]:\n",
        "        f.write(' '.join(s))\n",
        "        f.write('\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWVpmrVMlm0u"
      },
      "source": [
        "Обучаем и сохраняем модель. \n",
        "\n",
        "\n",
        "Основные параметры:\n",
        "* данные должны быть итерируемым объектом \n",
        "* size — размер вектора, \n",
        "* window — размер окна наблюдения,\n",
        "* min_count — мин. частотность слова в корпусе,\n",
        "* sg — используемый алгоритм обучения (0 — CBOW, 1 — Skip-gram),\n",
        "* sample — порог для downsampling'a высокочастотных слов,\n",
        "* workers — количество потоков,\n",
        "* alpha — learning rate,\n",
        "* iter — количество итераций,\n",
        "* max_vocab_size — позволяет выставить ограничение по памяти при создании словаря (т.е. если ограничение превышается, то низкочастотные слова будут выбрасываться). Для сравнения: 10 млн слов = 1Гб RAM.\n",
        "\n",
        "**NB!** Обратите внимание, что тренировка модели не включает препроцессинг! Это значит, что избавляться от пунктуации, приводить слова к нижнему регистру, лемматизировать их, проставлять частеречные теги придется до тренировки модели (если, конечно, это необходимо для вашей задачи). Т.е. в каком виде слова будут в исходном тексте, в таком они будут и в модели."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TEECgAnQlppC"
      },
      "source": [
        "print(\"Training model...\")\n",
        "# обучаем модель с векторами размерности 300, длиной окна 10\n",
        "%time model_en = word2vec.Word2Vec(sentences, workers=4, size=300, min_count=10, window=10, sample=1e-3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kIOEvpIlt3K"
      },
      "source": [
        "Смотрим, сколько в модели слов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM8Cpervlv4_"
      },
      "source": [
        "print(len(model_en.wv.vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvSW75bbl5OB"
      },
      "source": [
        "Попробуем оценить модель вручную, порешав примеры. Несколько дано ниже, попробуйте придумать свои."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpOa55MXl0br"
      },
      "source": [
        "print(model_en.wv.most_similar(positive=[\"woman\", \"actor\"], negative=[\"man\"], topn=1))\n",
        "print(model_en.wv.most_similar(positive=[\"dogs\", \"man\"], negative=[\"dog\"], topn=1))\n",
        "\n",
        "print(model_en.wv.most_similar(\"usa\", topn=3))\n",
        "\n",
        "print(model_en.wv.doesnt_match(\"comedy thriller western novel\".split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJoBb8olmGYQ"
      },
      "source": [
        "### Как дообучить существующую модель\n",
        "\n",
        "При тренировке модели \"с нуля\" веса инициализируются случайно, однако, можно использовать для инициализации векторов веса из предобученной модели, таким образом как бы дообучая ее.\n",
        "\n",
        "Сначала посмотрим близость какой-нибудь пары слов в имеющейся модели, чтобы потом сравнить результат с дообученной."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "171hCwa6mJiz"
      },
      "source": [
        "model_en.wv.similarity('lion', 'rabbit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYpQqzcamNL1"
      },
      "source": [
        "В качестве дополнительных данных для обучения возьмем английский текст «Алисы в Зазеркалье»."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YMEwE7OimPe9"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/train/alice.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sV2bGBFTmT0z"
      },
      "source": [
        "with open(\"alice.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# убираем переносы строк, токенизируем текст\n",
        "\n",
        "text = re.sub('\\n', ' ', text)\n",
        "sents = sent_tokenize(text)\n",
        "\n",
        "punct = '!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~„“«»†*—/\\-‘’'\n",
        "clean_sents = []\n",
        "\n",
        "# убираем всю пунктуацию и делим текст на слова по пробелу\n",
        "for sent in sents:\n",
        "    s = [w.lower().strip(punct) for w in sent.split()]\n",
        "    clean_sents.append(s)\n",
        "    \n",
        "print(clean_sents[:2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2O6QnwcmZYo"
      },
      "source": [
        "Чтобы дообучить модель, надо сначала ее сохранить, а потом загрузить. Все параметры тренировки (размер вектора, мин. частота слова и т.п.) будут взяты из загруженной модели, т.е. задать их заново нельзя.\n",
        "\n",
        "**NB!** Дообучить можно только полную модель (сохраненные при обучении веса и параметры модели, то есть обект самой модели), а `KeyedVectors` (просто пары \"слово - вектор\") — нельзя. Поэтому сохранять модель нужно в соотвествующем формате. Подробнее о разнице [вот тут](https://radimrehurek.com/gensim/models/keyedvectors.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z28eER7mc-Y"
      },
      "source": [
        "model_path = \"movie_reviews.model\"\n",
        "\n",
        "# так можно сохранить модель для последующего дообучения\n",
        "print(\"Saving model...\")\n",
        "model_en.save(model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-i0jCyGme7C"
      },
      "source": [
        "# загружаем нашу обученную модель и дообучаем на текстах \"Алисы\"\n",
        "\n",
        "model = word2vec.Word2Vec.load(model_path)\n",
        "\n",
        "model.build_vocab(clean_sents, update=True)\n",
        "model.train(clean_sents, total_examples=model.corpus_count, epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_pY1ftSImhv1"
      },
      "source": [
        "model.wv.similarity('lion', 'rabbit')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b02DdZigmkWw"
      },
      "source": [
        "Лев и кролик стали ближе друг к другу!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLOxX4yUmsPU"
      },
      "source": [
        "Можно нормализовать вектора, тогда модель будет занимать меньше RAM. Однако после этого её нельзя дотренировывать. Здесь используется L2-нормализация: вектора нормализуются так, что если сложить квадраты всех элементов вектора, в сумме получится 1. \n",
        "\n",
        "Кроме того, сохраним не полные вектора, а `KeyedVectors`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyObwn1GmuuS"
      },
      "source": [
        "model.init_sims(replace=True)\n",
        "model_path = \"movies_alice.bin\"\n",
        "\n",
        "print(\"Saving model...\")\n",
        "model_en.wv.save_word2vec_format(model_path, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAgKkUuVnD83"
      },
      "source": [
        "## Оценка\n",
        "\n",
        "Задача обучения модели w2v - это usupervised задача (обучение без учителя), \"правильных\" ответов нет, поэтому нельзя вычислить некую метрику качества, чтобы сравнить две модели между собой или просто по значению одной метрики сказать, насколько хороша полученная модель. \n",
        "\n",
        "Тем не менее, существуют специальные выборки для оценки качества дистрибутивных моделей. Основных два: один измеряет точность решения задач на аналогии (пример про Россию и пельмени), а второй используется для оценки коэффициента семантической близости. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTpG9KjE9eUQ"
      },
      "source": [
        "### Аналогии\n",
        "\n",
        "Другая популярная задача для \"внутренней\" оценки называется задачей поиска аналогий. Как мы уже разбирали выше, с помощью простых арифметических операций мы можем модифицировать значение слова. Если заранее собрать набор слов-модификаторов, а также слов, которые мы хотим получить в результаты модификации, то на основе подсчёта количества \"попаданий\" в желаемое слово мы можем оценить, насколько хорошо работает модель.\n",
        "\n",
        "В качестве слов-модификаторов мы можем использовать семантические аналогии. Скажем, если у нас есть некоторое отношение \"страна-столица\", то для оценки модели мы можем использовать пары наподобие \"Россия-Москва\", \"Норвегия-Осло\", и т.д. Выборка будет выглядеть следующм образом:\n",
        "\n",
        "| слово 1    | слово 2    | отношение     | \n",
        "|------------|------------|---------------|\n",
        "| Россия     | Москва     | страна-столица|  \n",
        "| Норвегия   | Осло       | страна-столица|\n",
        "\n",
        "Рассматривая случайные две пары из этого набора, мы хотим, имея триплет (Россия, Москва, Норвегия), получить слово \"Осло\", т.е. найти такое слово, которое будет находиться в том же отношении со словом \"Норвегия\", как \"Россия\" находится с Москвой. \n",
        "\n",
        "Выборки для русского языка можно скачать на странице с моделями на RusVectores. Посчитаем качество нашей модели НКРЯ на выборке про аналогии:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxRUo3g_9hw0"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/ancatmara/data-science-nlp/master/data/w2v/evaluation/ru_analogy_tagged.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUJHg-6v9lF6"
      },
      "source": [
        "with open('ru_analogy_tagged.txt','r') as f:\n",
        "  data = f.readlines()\n",
        "  print (data[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAHA6zn09lrP"
      },
      "source": [
        "res = model_ru.accuracy('ru_analogy_tagged.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bdf-h9Uf9r5N"
      },
      "source": [
        "print(res[4]['incorrect'][:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBJ0_n2e9yct"
      },
      "source": [
        "### Word Similarity\n",
        "\n",
        "Этот метод заключается в том, чтобы оценить, насколько представления о семантической близости слов в модели соотносятся с \"представлениями\" людей.\n",
        "\n",
        "| слово 1    | слово 2    | близость | \n",
        "|------------|------------|----------|\n",
        "| кошка      | собака     | 0.7      |  \n",
        "| чашка      | кружка     | 0.9      |       \n",
        "\n",
        "Для каждой пары слов из заранее заданного датасета мы можем посчитать косинусное расстояние, и получить список таких значений близости. При этом у нас уже есть список значений близостей, сделанный людьми. Мы можем сравнить эти два списка и понять, насколько они похожи (например, посчитав корреляцию). Эта мера схожести должна говорить о том, насколько модель хорошо моделирует расстояния до слова.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsnuE_CgnJO3"
      },
      "source": [
        "## Бонус 3. FastText\n",
        "\n",
        "FastText использует не только эмбеддинги слов, но и эмбеддинги n-грам. В корпусе каждое слово автоматически представляется в виде набора символьных n-грамм. \n",
        "\n",
        "Скажем, если мы установим n=3, то вектор для слова \"where\" будет представлен суммой векторов следующих триграм: \"<wh\", \"whe\", \"her\", \"ere\", \"re>\" (где \"<\" и \">\" символы, обозначающие начало и конец слова). \n",
        "\n",
        "Благодаря этому мы можем также получать вектора для слов, отсутствуюших в словаре, а также эффективно работать с текстами, содержащими ошибки и опечатки.\n",
        "\n",
        "* [Статья](https://aclweb.org/anthology/Q17-1010)\n",
        "* [Сайт](https://fasttext.cc/)\n",
        "* [Тьюториал](https://fasttext.cc/docs/en/support.html)\n",
        "* [Вектора для 157 языков](https://fasttext.cc/docs/en/crawl-vectors.html)\n",
        "* [Вектора, обученные на википедии](https://fasttext.cc/docs/en/pretrained-vectors.html) (отдельно для 294 разных языков)\n",
        "* [Репозиторий](https://github.com/facebookresearch/fasttext)\n",
        "\n",
        "Есть библиотека `fasttext` для питона (с готовыми моделями можно работать и через `gensim`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UXvgavDnLlx"
      },
      "source": [
        "# чтобы установить fasstext, можно склонировать его с репозитория \n",
        "! git clone https://github.com/facebookresearch/fastText.git\n",
        "! pip3 install fastText/."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GerzfRvLnQfC"
      },
      "source": [
        "import fasttext\n",
        "\n",
        "ft_model = fasttext.train_unsupervised('clean_text.txt', minn=3, maxn=4, dim=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL0w3IrUnTr_"
      },
      "source": [
        "ft_model.get_nearest_neighbors('actor')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Akwp4geNnWMh"
      },
      "source": [
        "ft_model.get_analogies(\"woman\", \"man\", \"actor\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_yBTNiTnbQj"
      },
      "source": [
        "ft_model.get_nearest_neighbors('actr')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyk-qki4njoE"
      },
      "source": [
        "ft_model.get_nearest_neighbors('moviegeek')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igRgYdQFzU62"
      },
      "source": [
        "Дополнение: https://github.com/dipanjanS/text-analytics-with-python/tree/master/New-Second-Edition"
      ]
    }
  ]
}